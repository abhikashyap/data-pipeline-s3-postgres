{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a60bdf06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/09 00:34:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1VHNcEG4EExsROyhb4eTU0yl4JYktNOrBW9VeUCU25cw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.8/site-packages/pygsheets/worksheet.py:1554: UserWarning: At least one column name in the data frame is an empty string. If this is a concern, please specify include_tailing_empty=False and/or ensure that each column containing data has a name.\n",
      "  warnings.warn('At least one column name in the data frame is an empty string. If this is a concern, please specify include_tailing_empty=False and/or ensure that each column containing data has a name.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1Qkb_jo6DkEFlL8lnP2b3IVgUf8H2ww21TaoQgNTSJAE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.8/site-packages/pygsheets/worksheet.py:1554: UserWarning: At least one column name in the data frame is an empty string. If this is a concern, please specify include_tailing_empty=False and/or ensure that each column containing data has a name.\n",
      "  warnings.warn('At least one column name in the data frame is an empty string. If this is a concern, please specify include_tailing_empty=False and/or ensure that each column containing data has a name.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1CxhjF6Rz4dg6udRjMCVehBk2cKO2B-U46SAmqmcgCyI\n",
      "1R1yAkW0C1NC_H2512AEI15BooNORv7Ad6Bs5EAdFRTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.8/site-packages/pygsheets/worksheet.py:1554: UserWarning: At least one column name in the data frame is an empty string. If this is a concern, please specify include_tailing_empty=False and/or ensure that each column containing data has a name.\n",
      "  warnings.warn('At least one column name in the data frame is an empty string. If this is a concern, please specify include_tailing_empty=False and/or ensure that each column containing data has a name.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15fn4kLCOU0YRT6ggyPX8s170-UQOFmEXpEmAln-OKCI\n",
      "1CiWc0CHuf8WtEOKg_nVRRijtNBRb60ardmVcgUZsp9A\n",
      "1fL0F1CQgkRGWW_iu0pY3cf5SOFQFw_ZbAqyDmvWRpgY\n",
      "jhqwdf \n",
      "1nv43vtcK6tNlXxmHDj2dySt646i4jG1JyyiTz6b8hUo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ip='172.31.19.202'\n",
    "db='amazon_sp_api'\n",
    "password='Techblooprint123'\n",
    "user=\"blooprint\"\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from clickhouse_driver import Client\n",
    "import datetime\n",
    "from pyspark.sql.types import FloatType\n",
    "from functools import reduce\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"BigCSVWrite\") \\\n",
    "#     .config(\"spark.driver.memory\", \"5g\") \\\n",
    "#     .config(\"spark.executor.memory\", \"6g\") \\\n",
    "#     .getOrCreate()\n",
    "# spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "# spark.conf.set(\"spark.sql.shuffle.partitions\", \"50\")\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"BigCSVWrite\")\n",
    "    .config(\"spark.driver.memory\", \"6g\")\n",
    "    .config(\"spark.executor.memory\", \"8g\")\n",
    "    .config(\"spark.executor.memoryOverhead\", \"2g\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"400\")\n",
    "    .config(\"spark.default.parallelism\", \"400\")\n",
    "    .config(\"spark.memory.fraction\", \"0.5\")\n",
    "    .config(\"spark.memory.storageFraction\", \"0.3\")\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\")\n",
    "    .config(\"spark.memory.offHeap.size\", \"2g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "from pyspark.sql import DataFrame  # or: from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql import functions as F, types as T\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "# Should not raise:\n",
    "spark.sparkContext._jvm.java.lang.Class.forName(\"org.postgresql.Driver\")\n",
    "\n",
    "URL  = os.getenv(\"PG_JDBC_URL\", f\"jdbc:postgresql://{ip}:5433/{db}\")\n",
    "USER = os.getenv(\"PG_USER\", user)\n",
    "PWD  = os.getenv(\"PG_PASSWORD\", password)\n",
    "import pygsheets\n",
    "import os\n",
    "\n",
    "def read_sheet(spreadsheet_id,sheet_name,index=0):\n",
    "    try:\n",
    "        index=int(index)\n",
    "        index=index%12\n",
    "        service_file_path = os.path.join(os.getcwd(), f\"key{index}.json\")\n",
    "        gc = pygsheets.authorize(service_file=service_file_path)\n",
    "        sh = gc.open_by_key(spreadsheet_id)\n",
    "        selected_sheet = sh.worksheet_by_title(sheet_name)\n",
    "        df = selected_sheet.get_as_df()\n",
    "        # database.rename_columns(df)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"jhqwdf\",e)\n",
    "        pass\n",
    "base_sheet='1_zTZSaaAM0svrV26XoX59ZcTJ6MMcpSNV64UOvRDHeA'\n",
    "df=read_sheet(base_sheet,'AZ',2)\n",
    "all_sheet_ids=df['sheet_id'].to_list()\n",
    "all_target_df=[]\n",
    "for sheet_id in all_sheet_ids:\n",
    "    print(sheet_id)\n",
    "    target_df=read_sheet(sheet_id,'Target ACOS AZ')\n",
    "    if target_df is not None:\n",
    "        target_df=target_df[['campaign_name','Target ACOS']].drop_duplicates()\n",
    "        all_target_df.append(target_df)\n",
    "\n",
    "all_campaign_target=pd.concat(all_target_df)\n",
    "all_campaign_target.to_csv(\"all_camp_target.csv\",index=False)\n",
    "# 2. Read CSV file\n",
    "target_df = spark.read.csv(\n",
    "    \"all_camp_target.csv\",\n",
    "    header=True,      # use first row as column names\n",
    "    inferSchema=True  # infer data types automatically\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "(\n",
    "    target_df.write\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"url\", URL)\n",
    "    .option(\"user\", USER)\n",
    "    .option(\"password\", PWD)\n",
    "    .option(\"driver\", \"org.postgresql.Driver\")\n",
    "    .option(\"dbtable\", \"public.target_info\")  # destination table\n",
    "    .mode(\"overwrite\")  # creates table if not exists, replaces if exists\n",
    "    .save()\n",
    ")\n",
    "os.remove('all_camp_target.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "855376fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_query = \"(SELECT * FROM public.portfolio_id_name_mapping) AS p\"\n",
    "\n",
    "portfolio_id_name_mapping = (spark.read.format(\"jdbc\")\n",
    "      .option(\"url\", URL)\n",
    "      .option(\"user\", USER)\n",
    "      .option(\"password\", PWD)\n",
    "      .option(\"driver\", \"org.postgresql.Driver\")\n",
    "      .option(\"dbtable\", portfolio_query)\n",
    "      .load())\n",
    "campaign_query = \"(SELECT * FROM public.campaigns) as c\"\n",
    "campaigns = (spark.read.format(\"jdbc\")\n",
    "      .option(\"url\", URL)\n",
    "      .option(\"user\", USER)\n",
    "      .option(\"password\", PWD)\n",
    "      .option(\"driver\", \"org.postgresql.Driver\")\n",
    "      .option(\"dbtable\", campaign_query)\n",
    "      .load())\n",
    "competitor_query = \"(SELECT * FROM public.competitor_brand_data) as cb\"\n",
    "competitors = (spark.read.format(\"jdbc\")\n",
    "      .option(\"url\", URL)\n",
    "      .option(\"user\", USER)\n",
    "      .option(\"password\", PWD)\n",
    "      .option(\"driver\", \"org.postgresql.Driver\")\n",
    "      .option(\"dbtable\", competitor_query)\n",
    "      .load())\n",
    "sb_kw_query = \"(SELECT * FROM public.sponsored_brands_search_term) as sb\"\n",
    "sb_kw = (spark.read.format(\"jdbc\")\n",
    "      .option(\"url\", URL)\n",
    "      .option(\"user\", USER)\n",
    "      .option(\"password\", PWD)\n",
    "      .option(\"driver\", \"org.postgresql.Driver\")\n",
    "      .option(\"dbtable\", sb_kw_query)\n",
    "      .load())\n",
    "sp_kw_query = \"(SELECT * FROM public.sponsored_products_search_term) as sp\"\n",
    "sp_kw = (spark.read.format(\"jdbc\")\n",
    "      .option(\"url\", URL)\n",
    "      .option(\"user\", USER)\n",
    "      .option(\"password\", PWD)\n",
    "      .option(\"driver\", \"org.postgresql.Driver\")\n",
    "      .option(\"dbtable\", sp_kw_query)\n",
    "      .load())\n",
    "sd_kw_query = \"(SELECT * FROM public.sponsored_display_targeting) as sd\"\n",
    "sd_kw = (spark.read.format(\"jdbc\")\n",
    "      .option(\"url\", URL)\n",
    "      .option(\"user\", USER)\n",
    "      .option(\"password\", PWD)\n",
    "      .option(\"driver\", \"org.postgresql.Driver\")\n",
    "      .option(\"dbtable\", sd_kw_query)\n",
    "      .load())\n",
    "\n",
    "targets_query = \"(SELECT * FROM public.targets ) as t\"\n",
    "targets = (spark.read.format(\"jdbc\")\n",
    "      .option(\"url\", URL)\n",
    "      .option(\"user\", USER)\n",
    "      .option(\"password\", PWD)\n",
    "      .option(\"driver\", \"org.postgresql.Driver\")\n",
    "      .option(\"dbtable\", targets_query)\n",
    "      .load())\n",
    "bid_query=\"\"\"(SELECT unnamed_subquery.keyword_id,\n",
    "            unnamed_subquery.bid,\n",
    "            unnamed_subquery.bid_tym\n",
    "           FROM ( SELECT bid_values.keyword_id,\n",
    "                    bid_values.bid,\n",
    "                    bid_values.updated_date::date AS bid_tym,\n",
    "                    rank() OVER (PARTITION BY bid_values.keyword_id ORDER BY bid_values.updated_date DESC) AS rank\n",
    "                   FROM bid_values) unnamed_subquery\n",
    "          WHERE unnamed_subquery.rank = 1 ) bid\"\"\"\n",
    "bid_df = (spark.read.format(\"jdbc\")\n",
    "      .option(\"url\", URL)\n",
    "      .option(\"user\", USER)\n",
    "      .option(\"password\", PWD)\n",
    "      .option(\"driver\", \"org.postgresql.Driver\")\n",
    "      .option(\"dbtable\", bid_query)\n",
    "      .load())\n",
    "ads_query = \"(SELECT * FROM public.ads ) as a\"\n",
    "ads_df_raw = (spark.read.format(\"jdbc\")\n",
    "      .option(\"url\", URL)\n",
    "      .option(\"user\", USER)\n",
    "      .option(\"password\", PWD)\n",
    "      .option(\"driver\", \"org.postgresql.Driver\")\n",
    "      .option(\"dbtable\", ads_query)\n",
    "      .load())\n",
    "\n",
    "campaign_df=campaigns.select(F.col(\"campaignId\").alias(\"campaign_id\"),\n",
    "                 F.col(\"portfolioId\").alias(\"portfolioId_camp\"),\n",
    "                 F.col(\"name\").alias(\"campaign_name\"),\n",
    "                 ).dropDuplicates(['campaign_id'])\n",
    "\n",
    "portfolio_df=portfolio_id_name_mapping.select(F.col(\"portfolioId\").alias(\"portfolioId\"),\n",
    "                                 F.col(\"name\").alias(\"portfolio_name\")).dropDuplicates(['portfolioId'])\n",
    "\n",
    "\n",
    "\n",
    "portfolio_mapping=campaign_df.join(portfolio_df,on=campaign_df['portfolioId_camp']==portfolio_df['portfolioId'],how='left')\n",
    "\n",
    "sb_search_term=sb_kw.select(F.col(\"account_name\"),\n",
    "             F.split(F.col(\"campaignId\").cast(\"string\"),\"\\\\.\").getItem(0).alias(\"campaignId\"),\n",
    "             F.col(\"campaignName\").alias(\"campaign_name\"),\n",
    "             F.split(F.col(\"adGroupId\").cast(\"string\"),\"\\\\.\").getItem(0).alias(\"ad_group_id\"),\n",
    "             F.col(\"adGroupName\").alias(\"ad_group_name\"),\n",
    "             F.split(F.col(\"keywordId\").cast(\"string\"),\"\\\\.\").getItem(0).alias(\"keyword_id\"),\n",
    "             F.col(\"keywordText\").alias(\"keyword\"),\n",
    "             F.col(\"searchTerm\").alias(\"search_term\"),\n",
    "             F.col(\"date\").cast(\"date\").alias(\"date\"),\n",
    "             F.col(\"impressions\").alias(\"impressions\"),\n",
    "             F.col(\"clicks\").alias(\"clicks\"),\n",
    "             F.col(\"cost\").alias(\"ad_spend\"),\n",
    "             F.col(\"sales\").alias(\"ads_sale\"),\n",
    "             F.col(\"unitsSold\").alias(\"ads_units\"),\n",
    "             F.col(\"matchType\").alias(\"match_type\"),\n",
    "\n",
    "\n",
    "\n",
    "             ).withColumn(\"kt_type\",F.lit(\"keyword_id\"))\n",
    "sd_search_term=sd_kw.select(F.col(\"account_name\"),\n",
    "             F.split(F.col(\"campaignId\").cast(\"string\"),\"\\\\.\").getItem(0).alias(\"campaignId\"),\n",
    "             F.col(\"campaignName\").alias(\"campaign_name\"),\n",
    "             F.split(F.col(\"adGroupId\").cast(\"string\"),\"\\\\.\").getItem(0).alias(\"ad_group_id\"),\n",
    "             F.col(\"adGroupName\").alias(\"ad_group_name\"),\n",
    "             F.split(F.col(\"targetingId\").cast(\"string\"),\"\\\\.\").getItem(0).alias(\"keyword_id\"),\n",
    "             F.col(\"targetingText\").alias(\"keyword\"),\n",
    "             F.lit(\"not_applicable\").alias(\"search_term\"),\n",
    "             F.col(\"date\").cast(\"date\").alias(\"date\"),\n",
    "             F.col(\"impressions\").alias(\"impressions\"),\n",
    "             F.col(\"clicks\").alias(\"clicks\"),\n",
    "             F.col(\"cost\").alias(\"ad_spend\"),\n",
    "             F.col(\"sales\").alias(\"ads_sale\"),\n",
    "             F.col(\"unitsSold\").alias(\"ads_units\"),\n",
    "             F.lit(\"DSP TGT\").alias(\"match_type\"),\n",
    "\n",
    "             ).withColumn(\"kt_type\",F.lit(\"targeting_id\"))\n",
    "sp_search_term=sp_kw.select(F.col(\"account_name\"),\n",
    "             F.split(F.col(\"campaignId\").cast(\"string\"),\"\\\\.\").getItem(0).alias(\"campaignId\"),\n",
    "             F.col(\"campaignName\").alias(\"campaign_name\"),\n",
    "             F.split(F.col(\"adGroupId\").cast(\"string\"),\"\\\\.\").getItem(0).alias(\"ad_group_id\"),\n",
    "             F.col(\"adGroupName\").alias(\"ad_group_name\"),\n",
    "             F.split(F.col(\"keywordId\").cast(\"string\"),\"\\\\.\").getItem(0).alias(\"keyword_id\"),\n",
    "             F.col(\"targeting\").alias(\"keyword\"),\n",
    "             F.col(\"searchTerm\").alias(\"search_term\"),\n",
    "             F.col(\"date\").cast(\"date\").alias(\"date\"),\n",
    "             F.col(\"impressions\").alias(\"impressions\"),\n",
    "             F.col(\"clicks\").alias(\"clicks\"),\n",
    "             F.col(\"cost\").alias(\"ad_spend\"),\n",
    "             F.col(\"sales30d\").alias(\"ads_sale\"),\n",
    "             F.col(\"unitsSoldSameSku1d\").alias(\"ads_units\"),\n",
    "             F.col(\"matchType\").alias(\"match_type\"),\n",
    "\n",
    "\n",
    "\n",
    "             ).withColumn(\"kt_type\",F.lit(\"targeting_id\"))\n",
    "             \n",
    "keyword_df=sp_search_term.unionByName(sb_search_term).unionByName(sd_search_term)\n",
    "keyword_df=keyword_df.withColumn(\n",
    "    \"new_match_type\",\n",
    "    F.when(\n",
    "        (F.col(\"match_type\") == \"TARGETING_EXPRESSION\") & F.col(\"keyword\").like(\"%asin=%\"),\n",
    "        F.lit(\"PT EXACT\")\n",
    "    ).when(\n",
    "        (F.col(\"match_type\") == \"TARGETING_EXPRESSION\") & F.col(\"keyword\").like(\"%category=%\"),\n",
    "        F.lit(\"PT CAT\")\n",
    "    ).when(\n",
    "        (F.col(\"match_type\") == \"TARGETING_EXPRESSION\") & F.col(\"keyword\").like(\"%asin-expanded=%\"),\n",
    "        F.lit(\"PT EXP\")\n",
    "    ).when(\n",
    "        F.col(\"match_type\") == \"TARGETING_EXPRESSION_PREDEFINED\",\n",
    "        F.upper(F.col(\"keyword\"))\n",
    "    ).otherwise(F.col(\"match_type\"))\n",
    ")\n",
    "\n",
    "\n",
    "target_df = (\n",
    "    targets\n",
    "    .select(\n",
    "        F.col(\"campaignId\").alias(\"campaign_id\"),\n",
    "        F.col(\"targetType\").alias(\"targeting_type\"),\n",
    "        F.col(\"targetDetails_event\").alias(\"target_event\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"target_group\",\n",
    "        F.when(F.upper(F.col(\"targeting_type\")) == \"PRODUCT\", \"PT\")\n",
    "         .when(F.upper(F.col(\"targeting_type\")) == \"KEYWORD\", \"KT\")\n",
    "         .when(F.upper(F.col(\"targeting_type\")).isin(\"AUTO\", \"THEME\"), \"AT\")\n",
    "         .when(F.upper(F.col(\"targeting_type\")) == \"PRODUCT_CATEGORY\", \"CT\")\n",
    "         .when(F.upper(F.col(\"targeting_type\")) == \"AUDIENCE\", \"ADT\")\n",
    "         .when(\n",
    "             (F.upper(F.col(\"targeting_type\")).isin(\"PRODUCT_CATEGORY_AUDIENCE\", \"PRODUCT_AUDIENCE\")) &\n",
    "             (F.upper(F.col(\"target_event\")) == \"PURCHASES\"),\n",
    "             \"PR\"\n",
    "         )\n",
    "         .when(\n",
    "             (F.upper(F.col(\"targeting_type\")).isin(\"PRODUCT_CATEGORY_AUDIENCE\", \"PRODUCT_AUDIENCE\")) &\n",
    "             (F.upper(F.col(\"target_event\")) == \"VIEWS\"),\n",
    "             \"VR\"\n",
    "         )\n",
    "         .otherwise(\"Not Applicable\")\n",
    "    )\n",
    "    .dropDuplicates()\n",
    "    .select(\"campaign_id\",\"target_group\")\n",
    ")\n",
    "target_df = target_df.dropDuplicates([\"campaign_id\"])\n",
    "bid_df = bid_df.dropDuplicates([\"keyword_id\"])\n",
    "keyword_df=keyword_df.join(\n",
    "    F.broadcast(bid_df),\n",
    "    on=\"keyword_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "portfolio_mapping=portfolio_mapping.join(target_df,on='campaign_id',how='left').withColumnRenamed(\"target_group\",\"targeting_type\")\n",
    "\n",
    "\n",
    "ads_df=(ads_df_raw.select(\n",
    "    F.col(\"account_name\").alias(\"ads_account_name\"),\n",
    "    F.col(\"campaignId\").alias(\"ads_campaign_id\"),\n",
    "    F.col(\"adProduct\").alias(\"ad_product\"),\n",
    "    F.col(\"adType\").alias(\"ad_type\"),\n",
    "    F.col(\"state\").alias(\"CAMP STATUS\"),\n",
    "    F.col(\"creative_products\")\n",
    ").dropDuplicates()\n",
    ".withColumn(\"ads_type\",\n",
    "    F.when(F.col(\"ad_product\")==\"SPONSORED_PRODUCTS\",\"SP\")\n",
    "    .when((F.col(\"ad_product\")==\"SPONSORED_BRANDS\")&(F.col(\"ad_type\").isin(\"BRAND_VIDEO\",\"VIDEO\")),\"SBV\")\n",
    "    .when((F.col(\"ad_product\")==\"SPONSORED_BRANDS\")&(F.col(\"ad_type\")==\"PRODUCT_COLLECTION\"),\"SBB\")\n",
    "    .when((F.col(\"ad_product\")==\"SPONSORED_BRANDS\")&(F.col(\"ad_type\")==\"STORE_SPOTLIGHT\"),\"SBS\")\n",
    "    .when((F.col(\"ad_product\")==\"SPONSORED_DISPLAY\")&(F.col(\"ad_type\")==\"IMAGE\"),\"SDI\")\n",
    "    .when((F.col(\"ad_product\")==\"SPONSORED_DISPLAY\")&(F.col(\"ad_type\")==\"PRODUCT_AD\"),\"SDP\")\n",
    "    .when((F.col(\"ad_product\")==\"SPONSORED_DISPLAY\")&(F.col(\"ad_type\")==\"VIDEO\"),\"SDV\")\n",
    "    .otherwise(\"Not Applicable\")          \n",
    ").select(\n",
    "    \"ads_account_name\",\"ads_campaign_id\",\"ads_type\",\"CAMP STATUS\",\"creative_products\"\n",
    ")\n",
    ")\n",
    "\n",
    "ads_df = ads_df.dropDuplicates([\"ads_campaign_id\"])\n",
    "keyword_df=keyword_df.join(\n",
    "    F.broadcast(ads_df),\n",
    "    (keyword_df[\"account_name\"] == ads_df[\"ads_account_name\"]) &\n",
    "    (keyword_df[\"campaignId\"] == ads_df[\"ads_campaign_id\"]),\n",
    "    how=\"inner\"\n",
    ").drop(\"ads_account_name\",\"ads_campaign_id\")\n",
    "\n",
    "\n",
    "kw_meta_data=keyword_df.select(\n",
    "    \"account_name\",\"ads_type\",\"campaignId\",\"ad_group_id\",\"ad_group_name\",\"kt_type\",\"keyword_id\",\"keyword\",\"new_match_type\",\"bid\",\"bid_tym\",\"creative_products\"\n",
    ").dropDuplicates()\n",
    "keyword_df=keyword_df.groupby(\"account_name\",\"campaignId\",\"ad_group_id\",\"ad_group_name\",\"kt_type\",\"keyword_id\",\"keyword\",\"new_match_type\",\"date\").agg(\n",
    "F.sum(\"impressions\").alias(\"impressions\"),\n",
    "F.sum(\"clicks\").alias(\"clicks\"),\n",
    "F.sum(\"ad_spend\").alias(\"ad_spend\"),\n",
    "F.sum(\"ads_sale\").alias(\"ads_sale\"),\n",
    "F.sum(\"ads_units\").alias(\"ads_units\")\n",
    ")\n",
    "keyword_df=keyword_df.withColumn(\"month\",\n",
    "    F.date_format(F.col(\"date\"),'MMM')\n",
    ").withColumn(\"year\",\n",
    "    F.year(F.col(\"date\"))\n",
    ")\n",
    "today = datetime.datetime.today() - datetime.timedelta(days = 0)\n",
    "yesterday = datetime.datetime.today() - datetime.timedelta(days = 1)\n",
    "last_7_days= datetime.datetime.today() - datetime.timedelta(days = 7)\n",
    "last_14_days= datetime.datetime.today() - datetime.timedelta(days = 14)\n",
    "last_30_days= datetime.datetime.today() - datetime.timedelta(days = 30)\n",
    "last_60_days= datetime.datetime.today() - datetime.timedelta(days = 60)\n",
    "today=today.strftime('%Y-%m-%d')\n",
    "yesterday = yesterday.strftime('%Y-%m-%d')\n",
    "last_7_days = last_7_days.strftime('%Y-%m-%d')\n",
    "last_14_days = last_14_days.strftime('%Y-%m-%d')\n",
    "last_30_days = last_30_days.strftime('%Y-%m-%d')\n",
    "last_60_days = last_60_days.strftime('%Y-%m-%d')\n",
    "# keyword_df=keyword_df.filter(F.col(\"date\") != today)\n",
    "\n",
    "\n",
    "portfolio_meta_data=kw_meta_data.join(F.broadcast(portfolio_mapping),how='left',on=kw_meta_data['campaignId']==portfolio_mapping['campaign_id'])\n",
    "portfolio_meta_data=portfolio_meta_data.drop('campaign_id','portfolioId_camp')\n",
    "portfolio_meta_data=portfolio_meta_data.dropDuplicates([\"account_name\",\"campaignId\",\"ad_group_id\",\"keyword_id\"])\n",
    "\n",
    "\n",
    "def calculate_metrics(i, mode=1):\n",
    "    if mode == 1:\n",
    "        if i == last_14_days:\n",
    "            d = 'last_14_days'\n",
    "        elif i == last_7_days:\n",
    "            d = 'last_7_days'\n",
    "        elif i == yesterday:\n",
    "            d = 'yesterday'\n",
    "        elif i == last_30_days:\n",
    "            d = 'last_30_days'\n",
    "        elif i == last_60_days:\n",
    "            d = 'last_60_days'\n",
    "        elif i == today:\n",
    "            d = 'today'\n",
    "            base_df = keyword_df.filter(F.col(\"date\") == today)\n",
    "\n",
    "        if i == yesterday:\n",
    "            base_df = keyword_df.filter(F.col(\"date\") == yesterday)\n",
    "        else:\n",
    "            base_df = keyword_df\n",
    "\n",
    "\n",
    "        temp_df = base_df.filter(F.col(\"date\") >= i)\n",
    "    else:\n",
    "        month = i[0]\n",
    "        year = i[1]\n",
    "        d = month\n",
    "\n",
    "        temp_df = keyword_df.filter(\n",
    "            (F.col('month') == month) & (F.col('year') == year)\n",
    "        )\n",
    "\n",
    "    agg_df = temp_df.groupby(\"account_name\", \"campaignId\", \"ad_group_id\", \"keyword_id\").agg(\n",
    "        F.round(F.sum(\"impressions\"), 0).alias(f\"impressions_{d}\"),\n",
    "        F.round(F.sum(\"clicks\"), 0).alias(f\"clicks_{d}\"),\n",
    "        F.round(F.sum(\"ad_spend\"), 0).alias(f\"ad_spend_{d}\"),\n",
    "        F.round(F.sum(\"ads_sale\"), 0).alias(f\"ads_sale_{d}\"),\n",
    "        F.round(F.sum(\"ads_units\"), 0).alias(f\"ads_units_{d}\")\n",
    "    )\n",
    "\n",
    "    agg_df = (\n",
    "        agg_df\n",
    "        .withColumn(\n",
    "            f\"CPC_{d}\",\n",
    "            F.when(F.col(f\"clicks_{d}\") != 0,\n",
    "                   F.round(F.col(f\"ad_spend_{d}\") / F.col(f\"clicks_{d}\"), 2))\n",
    "            .otherwise(F.lit(0))\n",
    "        )\n",
    "        .withColumn(\n",
    "            f\"CPA_{d}\",\n",
    "            F.when(F.col(f\"ads_units_{d}\") != 0,\n",
    "                   F.round(F.col(f\"ad_spend_{d}\") / F.col(f\"ads_units_{d}\"), 2))\n",
    "            .otherwise(F.lit(0))\n",
    "        )\n",
    "        .withColumn(\n",
    "            f\"ACOS_{d}\",\n",
    "            F.when(F.col(f\"ads_sale_{d}\") != 0,\n",
    "                   F.round((F.col(f\"ad_spend_{d}\") * 100) / F.col(f\"ads_sale_{d}\"), 2))\n",
    "            .otherwise(F.lit(0))\n",
    "        )\n",
    "        .withColumn(\n",
    "            f\"CVR_{d}\",\n",
    "            F.when(F.col(f\"clicks_{d}\") != 0,\n",
    "                   F.round((F.col(f\"ads_units_{d}\") * 100) / F.col(f\"clicks_{d}\"), 2))\n",
    "            .otherwise(F.lit(0))\n",
    "        )\n",
    "        .withColumn(\n",
    "            f\"CTR_{d}\",\n",
    "            F.when(F.col(f\"impressions_{d}\") != 0,\n",
    "                   F.round((F.col(f\"clicks_{d}\") * 100) / F.col(f\"impressions_{d}\"), 2))\n",
    "            .otherwise(F.lit(0))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # ✅ Replace nulls in numeric columns with 0\n",
    "    agg_df = agg_df.fillna(0)\n",
    "\n",
    "\n",
    "    return agg_df\n",
    "\n",
    "current_month = datetime.date.today().month\n",
    "\n",
    "\n",
    "dfs = []\n",
    "time_cut=[today,yesterday,last_7_days,last_14_days,last_30_days,last_60_days]\n",
    "for time_duration in time_cut:\n",
    "    df=calculate_metrics(time_duration, 1)\n",
    "    dfs.append(df)\n",
    "\n",
    "\n",
    "for m in range(current_month ,current_month - 4,-1):\n",
    "    month_name = datetime.date(1900, m, 1).strftime(\"%b\")\n",
    "\n",
    "    df = calculate_metrics([month_name, 2025], 2)\n",
    "    \n",
    "    dfs.append(df)\n",
    "\n",
    "\n",
    "\n",
    "if dfs:\n",
    "    final_df = reduce(\n",
    "        lambda left, right: left.join(\n",
    "            right,\n",
    "            on=[\"account_name\", \"campaignId\", \"ad_group_id\", \"keyword_id\"],\n",
    "            how=\"outer\"\n",
    "        ),\n",
    "        dfs\n",
    "    )\n",
    "\n",
    "final_df=final_df.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a3e44d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/09 00:36:27 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/11/09 00:39:27 WARN DAGScheduler: Broadcasting large task binary with size 1126.1 KiB\n",
      "25/11/09 00:39:33 WARN DAGScheduler: Broadcasting large task binary with size 1104.1 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "campaign_target_query = \"(SELECT * FROM public.target_info) AS p\"\n",
    "\n",
    "campaign_target_df = (spark.read.format(\"jdbc\")\n",
    "      .option(\"url\", URL)\n",
    "      .option(\"user\", USER)\n",
    "      .option(\"password\", PWD)\n",
    "      .option(\"driver\", \"org.postgresql.Driver\")\n",
    "      .option(\"dbtable\", campaign_target_query)\n",
    "      .load())\n",
    "# Keys to join on\n",
    "final_df = (\n",
    "    final_df\n",
    "    .withColumnRenamed(\"account_name\", \"account_name_kw\")\n",
    "\n",
    "    .withColumnRenamed(\"campaignId\", \"campaignId_kw\")\n",
    "    .withColumnRenamed(\"ad_group_id\", \"ad_group_id_kw\")\n",
    "    .withColumnRenamed(\"keyword_id\", \"keyword_id_kw\")\n",
    ")\n",
    "\n",
    "condition = (\n",
    "    (final_df['account_name_kw'] == portfolio_meta_data['account_name'])&\n",
    "    (final_df['campaignId_kw'] == portfolio_meta_data['campaignId']) &\n",
    "    (final_df['ad_group_id_kw'] == portfolio_meta_data['ad_group_id']) &\n",
    "    (final_df['keyword_id_kw'] == portfolio_meta_data['keyword_id'])\n",
    ")\n",
    "\n",
    "final_mapped_data = (\n",
    "    final_df\n",
    "    .join(\n",
    "        F.broadcast(portfolio_meta_data),\n",
    "        on=condition,\n",
    "        how=\"left\"   # keeps all rows from final_df\n",
    "    )\n",
    ")\n",
    "\n",
    "mapped_acos=(\n",
    "    final_mapped_data\n",
    "    .join(\n",
    "        F.broadcast(campaign_target_df),\n",
    "        on='campaign_name',   # join on multiple columns\n",
    "        how=\"left\"      # keep all rows from final_df\n",
    "    )\n",
    ")\n",
    "ads_automation_sheet='14aA8eKlsJqCd8mrNZcZOcnwW5pGICTZKs7DVE-IjO8Q'\n",
    "df=read_sheet(ads_automation_sheet,'Account_level_setting',2)\n",
    "df.to_csv(\"account_setting.csv\",index=False)\n",
    "account_setting=spark.read.csv(\n",
    "    \"account_setting.csv\",\n",
    "    header=True,      # use first row as column names\n",
    "    inferSchema=True  # infer data types automatically\n",
    ")\n",
    "account_setting_joined = mapped_acos.alias(\"m\").join(\n",
    "    account_setting.alias(\"a\"),\n",
    "    on=\"account_name\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "mapped_acos_ratio = (\n",
    "    account_setting_joined\n",
    "    .withColumn(\n",
    "        \"target_acos_new\",\n",
    "        F.coalesce(F.col(\"Target ACOS\"), F.col(\"target_acos\"))\n",
    "    )\n",
    "    .drop(\"Target ACOS\", \"target_acos\")\n",
    "    .withColumnRenamed(\"target_acos_new\", \"target_acos\")\n",
    "    .withColumn(\"target_acos\", F.col(\"target_acos\").cast(FloatType()))\n",
    ")\n",
    "df=mapped_acos_ratio\n",
    "from pyspark.sql import functions as F, Window\n",
    "w = Window.partitionBy(\"portfolio_name\",\"keyword_id\").orderBy(F.col(\"ads_sale_last_60_days\").desc())\n",
    "w_unordered = Window.partitionBy(\"portfolio_name\")\n",
    "df = df.withColumn(\"cumulative_sales\", F.sum(\"ads_sale_last_60_days\").over(w))\n",
    "df = df.withColumn(\"cumulative_sales\", F.sum(\"ads_sale_last_60_days\").over(w))\n",
    "\n",
    "# total sales per portfolio\n",
    "df = df.withColumn(\"total_sales_portfolio\", F.sum(\"ads_sale_last_60_days\").over(w_unordered))\n",
    "\n",
    "# cumulative percentage\n",
    "df = df.withColumn(\"cumulative_percent\", F.col(\"cumulative_sales\") / F.col(\"total_sales_portfolio\"))\n",
    "df = df.withColumn(\n",
    "    \"priority_index\",\n",
    "    F.when(F.col(\"ads_sale_last_60_days\") == 0, F.lit(-1)) # if sales = 0\n",
    "        .when(F.col(\"cumulative_percent\") <= 0.8, F.lit(1))   # top 80%\n",
    "        .otherwise(F.lit(0))                                  # rest\n",
    ")\n",
    "all_columns=df.columns\n",
    "columns_adjusted = [\n",
    "    'account_name',\n",
    "    'portfolioId',\n",
    "    'portfolio_name',\n",
    "    'ads_type',\n",
    "\n",
    "    'campaignId',\n",
    "    'campaign_name',\n",
    "    'ad_group_id',\n",
    "    'ad_group_name',\n",
    "    'kt_type',\n",
    "    'keyword_id',\n",
    "    'keyword',\n",
    "    'new_match_type',\n",
    "    'targeting_type', 'bid', 'bid_tym',\n",
    "    'creative_products',\n",
    "    'target_acos',\n",
    "    'priority_index',\n",
    "\n",
    "    # ---- Daily Metrics ----\n",
    "    'impressions_today',\n",
    "    'clicks_today',\n",
    "    'ad_spend_today',\n",
    "    'ads_sale_today',\n",
    "    'ads_units_today',\n",
    "    'CPC_today',\n",
    "    'CPA_today',\n",
    "    'ACOS_today',\n",
    "    'CVR_today',\n",
    "    'CTR_today',\n",
    "\n",
    "    'impressions_yesterday',\n",
    "    'clicks_yesterday',\n",
    "    'ad_spend_yesterday',\n",
    "    'ads_sale_yesterday',\n",
    "    'ads_units_yesterday',\n",
    "    'CPC_yesterday',\n",
    "    'CPA_yesterday',\n",
    "    'ACOS_yesterday',\n",
    "    'CVR_yesterday',\n",
    "    'CTR_yesterday',\n",
    "\n",
    "    # ---- Rolling Windows ----\n",
    "    'impressions_last_7_days',\n",
    "    'clicks_last_7_days',\n",
    "    'ad_spend_last_7_days',\n",
    "    'ads_sale_last_7_days',\n",
    "    'ads_units_last_7_days',\n",
    "    'CPC_last_7_days',\n",
    "    'CPA_last_7_days',\n",
    "    'ACOS_last_7_days',\n",
    "    'CVR_last_7_days',\n",
    "    'CTR_last_7_days',\n",
    "\n",
    "    'impressions_last_14_days',\n",
    "    'clicks_last_14_days',\n",
    "    'ad_spend_last_14_days',\n",
    "    'ads_sale_last_14_days',\n",
    "    'ads_units_last_14_days',\n",
    "    'CPC_last_14_days',\n",
    "    'CPA_last_14_days',\n",
    "    'ACOS_last_14_days',\n",
    "    'CVR_last_14_days',\n",
    "    'CTR_last_14_days',\n",
    "\n",
    "    'impressions_last_30_days',\n",
    "    'clicks_last_30_days',\n",
    "    'ad_spend_last_30_days',\n",
    "    'ads_sale_last_30_days',\n",
    "    'ads_units_last_30_days',\n",
    "    'CPC_last_30_days',\n",
    "    'CPA_last_30_days',\n",
    "    'ACOS_last_30_days',\n",
    "    'CVR_last_30_days',\n",
    "    'CTR_last_30_days',\n",
    "\n",
    "    'impressions_last_60_days',\n",
    "    'clicks_last_60_days',\n",
    "    'ad_spend_last_60_days',\n",
    "    'ads_sale_last_60_days',\n",
    "    'ads_units_last_60_days',\n",
    "    'CPC_last_60_days',\n",
    "    'CPA_last_60_days',\n",
    "    'ACOS_last_60_days',\n",
    "    'CVR_last_60_days',\n",
    "    'CTR_last_60_days',\n",
    "\n",
    "    # ---- Monthly Metrics ----\n",
    "    'impressions_Sep',\n",
    "    'clicks_Sep',\n",
    "    'ad_spend_Sep',\n",
    "    'ads_sale_Sep',\n",
    "    'ads_units_Sep',\n",
    "    'CPC_Sep',\n",
    "    'CPA_Sep',\n",
    "    'ACOS_Sep',\n",
    "    'CVR_Sep',\n",
    "    'CTR_Sep',\n",
    "\n",
    "    'impressions_Oct',\n",
    "    'clicks_Oct',\n",
    "    'ad_spend_Oct',\n",
    "    'ads_sale_Oct',\n",
    "    'ads_units_Oct',\n",
    "    'CPC_Oct',\n",
    "    'CPA_Oct',\n",
    "    'ACOS_Oct',\n",
    "    'CVR_Oct',\n",
    "    'CTR_Oct',\n",
    "\n",
    "    'impressions_Nov',\n",
    "    'clicks_Nov',\n",
    "    'ad_spend_Nov',\n",
    "    'ads_sale_Nov',\n",
    "    'ads_units_Nov',\n",
    "    'CPC_Nov',\n",
    "    'CPA_Nov',\n",
    "    'ACOS_Nov',\n",
    "    'CVR_Nov',\n",
    "    'CTR_Nov',\n",
    "\n",
    "    # ---- Summary ----\n",
    "    'cumulative_sales',\n",
    "    'total_sales_portfolio',\n",
    "    'cumulative_percent',\n",
    "]\n",
    "df=df.select(columns_adjusted)\n",
    "df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", URL) \\\n",
    "    .option(\"user\", USER) \\\n",
    "    .option(\"password\", PWD) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"dbtable\", \"public.kt_pt_ct_report\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afee6b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ads_automation_sheet='14aA8eKlsJqCd8mrNZcZOcnwW5pGICTZKs7DVE-IjO8Q'\n",
    "rules=read_sheet(ads_automation_sheet,'Rules',2)\n",
    "from sqlalchemy import (\n",
    "    create_engine,\n",
    "\n",
    "    text,\n",
    "\n",
    ")\n",
    "connection_string = (\n",
    "    f\"postgresql://{user}:{password}@{ip}:{5433}/{db}\"\n",
    ")\n",
    "\n",
    "def query_to_df(db_url,sql_query):\n",
    "    db_url = db_url\n",
    "\n",
    "    engine = create_engine(db_url)\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(text(sql_query))\n",
    "        \n",
    "        # Fetch the results\n",
    "        rows = result.fetchall()\n",
    "        \n",
    "        df=pd.DataFrame(rows)\n",
    "    return df\n",
    "query=\"\"\" \n",
    "select * from kt_pt_ct_report where account_name = 'awenest'\n",
    "\"\"\"\n",
    "df=query_to_df(connection_string,query)\n",
    "df[\"ACOS_last_14_days\"] = df[\"ACOS_last_14_days\"].astype(float)\n",
    "df['Acos_ratio']= df['ACOS_last_14_days'].div(df['target_acos']).astype(\"float\").round(2)\n",
    "account_rules=rules[rules['account_name']=='awenest']\n",
    "\n",
    "rulesdf = account_rules.replace(\"\", pd.NA)\n",
    "end_cols=[col for col in rulesdf.columns if 'end' in col]\n",
    "all_nums=[]\n",
    "for each_col in end_cols:\n",
    "    num=int(each_col.split(\"_\")[0])\n",
    "    all_nums.append(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9529babd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Rule 1: updated 2137 rows → bid_change='10'\n",
      "✅ Rule 2: updated 1576 rows → bid_change='10'\n",
      "✅ Rule 3: updated 55 rows → bid_change='5'\n",
      "✅ Rule 4: updated 40 rows → bid_change='2'\n",
      "✅ Rule 5: updated 0 rows → bid_change='-50'\n",
      "✅ Rule 6: updated 0 rows → bid_change='10'\n",
      "✅ Rule 7: updated 0 rows → bid_change='no change'\n",
      "✅ Rule 8: updated 184 rows → bid_change='cpc/ratio'\n",
      "\n",
      "Total rows with suggested_bid: 3992 / 3992\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Fetch data from Postgres with column names ---\n",
    "def query_to_df(db_url, sql_query):\n",
    "    engine = create_engine(db_url)\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(text(sql_query))\n",
    "        rows = result.fetchall()\n",
    "        cols = result.keys()              # ✅ fetch column names\n",
    "        df = pd.DataFrame(rows, columns=cols)\n",
    "    return df\n",
    "\n",
    "\n",
    "# --- Load data ---\n",
    "query = \"\"\"select * from kt_pt_ct_report where account_name = 'awenest'\"\"\"\n",
    "df = query_to_df(connection_string, query)\n",
    "import decimal\n",
    "\n",
    "# Convert all Decimal columns to float\n",
    "for col in df.columns:\n",
    "    if df[col].apply(lambda x: isinstance(x, decimal.Decimal)).any():\n",
    "        df[col] = df[col].astype(float)\n",
    "\n",
    "df['Acos_ratio'] = df['ACOS_last_14_days'].div(df['target_acos']).astype(float).round(2)\n",
    "\n",
    "# --- Prepare rules ---\n",
    "account_rules = rules[rules['account_name'] == 'awenest']\n",
    "rulesdf = account_rules.replace(\"\", pd.NA).copy()\n",
    "\n",
    "end_cols = [c for c in rulesdf.columns if c.endswith(\"_end\")]\n",
    "all_nums = [int(c.split(\"_\")[0]) for c in end_cols]\n",
    "max_col_index = max(all_nums)\n",
    "\n",
    "df[\"suggested_bid\"] = np.nan\n",
    "df[\"_portfolio_lower\"] = df[\"portfolio_name\"].fillna(\"\").str.strip().str.lower()\n",
    "\n",
    "# --- Apply all rules sequentially ---\n",
    "for idx, rule in rulesdf.iterrows():\n",
    "    mask = pd.Series(True, index=df.index)\n",
    "\n",
    "    # 1️⃣ Portfolio filter\n",
    "    portfolios_raw = rule.get(\"portfolio_name\")\n",
    "    if pd.notna(portfolios_raw) and str(portfolios_raw).strip():\n",
    "        allowed = [p.strip().lower() for p in str(portfolios_raw).split(\",\") if p.strip()]\n",
    "        mask &= df[\"_portfolio_lower\"].isin(allowed)\n",
    "    # else blank = all portfolios\n",
    "\n",
    "    # 2️⃣ Column filters\n",
    "    for i in range(1, max_col_index + 1):\n",
    "        col_filter = rule.get(f\"{i}_column_filter\")\n",
    "        start = rule.get(f\"{i}_start\")\n",
    "        end = rule.get(f\"{i}_end\")\n",
    "\n",
    "        # Skip blank sets\n",
    "        if pd.isna(col_filter) and pd.isna(start) and pd.isna(end):\n",
    "            continue\n",
    "        if pd.isna(col_filter) or col_filter not in df.columns:\n",
    "            continue\n",
    "\n",
    "        df[col_filter] = pd.to_numeric(df[col_filter], errors=\"coerce\").fillna(0)\n",
    "\n",
    "        if pd.notna(start) and pd.notna(end):\n",
    "            mask &= (df[col_filter] >= float(start)) & (df[col_filter] < float(end))\n",
    "        elif pd.notna(start):\n",
    "            mask &= (df[col_filter] >= float(start))\n",
    "        elif pd.notna(end):\n",
    "            mask &= (df[col_filter] < float(end))\n",
    "\n",
    "    if not mask.any():\n",
    "        print(f\"❌ Rule {idx+1}: no matches → skipped\")\n",
    "        continue\n",
    "\n",
    "    cpc_col = rule[\"cpc_column_name\"]\n",
    "    bid_change = str(rule[\"bid_change\"]).strip().lower()\n",
    "    if cpc_col not in df.columns:\n",
    "        print(f\"⚠️ Rule {idx+1}: CPC column '{cpc_col}' not found → skipped\")\n",
    "        continue\n",
    "\n",
    "    target_mask = mask & df[\"suggested_bid\"].isna()   # only update empty bids\n",
    "\n",
    "    if bid_change == \"no change\":\n",
    "        df.loc[target_mask, \"suggested_bid\"] = df.loc[target_mask, cpc_col]\n",
    "\n",
    "    elif bid_change == \"cpc/ratio\":\n",
    "        df.loc[target_mask, \"suggested_bid\"] = np.where(\n",
    "            df.loc[target_mask, \"Acos_ratio\"] > 0,\n",
    "            df.loc[target_mask, cpc_col] / df.loc[target_mask, \"Acos_ratio\"].replace(0, np.nan),\n",
    "            df.loc[target_mask, cpc_col]\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            pct = float(bid_change)\n",
    "            factor = 1 + (pct / 100.0)\n",
    "            df.loc[target_mask, \"suggested_bid\"] = df.loc[target_mask, cpc_col].astype(float) * factor\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    print(f\"✅ Rule {idx+1}: updated {target_mask.sum()} rows → bid_change='{bid_change}'\")\n",
    "\n",
    "# --- Cleanup ---\n",
    "df.drop(columns=[\"_portfolio_lower\"], inplace=True)\n",
    "print(f\"\\nTotal rows with suggested_bid: {df['suggested_bid'].notna().sum()} / {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a112d86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d9ea3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9544a45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5785303c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sheet(index,final_result, sheet_name, sheet_id, x, y, mode=1,make_upper_case=0):\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "    current_time = datetime.now()\n",
    "    current_date_string = current_time.strftime('%Y-%m-%d')\n",
    "    if \"updated_date\" not in final_result.columns:\n",
    "        final_result['updated_date'] = current_date_string\n",
    "    index=int(index)\n",
    "    index=index%12\n",
    "    # time.sleep(4)\n",
    "    service_file_path = os.path.join(os.getcwd(), f\"key{index}.json\")\n",
    "    print(service_file_path)\n",
    "    gc = pygsheets.authorize(service_file=service_file_path)\n",
    "    sh=gc.open_by_key(sheet_id)\n",
    "    selected_sheet = sh.worksheet_by_title(sheet_name)\n",
    "    if make_upper_case==1:\n",
    "        column_names = final_result.columns.tolist()\n",
    "        converted_columns = []\n",
    "        for column_name in column_names:\n",
    "            column_name = column_name.split('_')\n",
    "            column_name = list(map(lambda x:x.capitalize(), column_name))\n",
    "            column_name = (' ').join(column_name)\n",
    "            converted_columns.append(column_name)\n",
    "        names_mapping = dict(zip(column_names, converted_columns))\n",
    "        final_result = final_result.rename(columns=names_mapping)\n",
    "\n",
    "    \n",
    "\n",
    "    \"\"\" \n",
    "    Mode 2 will now append data while keeping the headers intact and delete the column names row for the new data.\n",
    "    Mode 2 adds only 2 new rows\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Mode 3 will also add data while maintaining headers, but will delete the column names row for the new data.\n",
    "    Mode 3 adds the entire lenght of the new dataframe + 2 rows\n",
    "    \"\"\"\n",
    "    if mode == 1:\n",
    "        selected_sheet.set_dataframe(final_result, (x, y), fit=True)\n",
    "\n",
    "    elif mode == 2:\n",
    "        last_filled_row = len(selected_sheet.get_as_df())\n",
    "        selected_sheet.add_rows(2)\n",
    "        print_from = last_filled_row + 2\n",
    "\n",
    "        if last_filled_row == 0:\n",
    "            selected_sheet.set_dataframe(final_result, (1, 1))\n",
    "\n",
    "        else:\n",
    "            selected_sheet.set_dataframe(final_result, (print_from, 1))\n",
    "            selected_sheet.delete_rows(print_from)\n",
    "\n",
    "\n",
    "    elif mode == 3:\n",
    "        last_filled_row = len(selected_sheet.get_as_df())\n",
    "        total_rows_needed = last_filled_row + len(final_result) + 2\n",
    "        existing_rows = selected_sheet.rows\n",
    "\n",
    "        # Add rows only if needed\n",
    "        if total_rows_needed > existing_rows:\n",
    "            selected_sheet.add_rows(total_rows_needed - existing_rows)\n",
    "\n",
    "        time.sleep(3)\n",
    "        print_from = last_filled_row + 2 if last_filled_row > 0 else 1\n",
    "        selected_sheet.set_dataframe(final_result, (print_from, 1))\n",
    "\n",
    "        if last_filled_row > 0:\n",
    "            selected_sheet.delete_rows(print_from)\n",
    "    else:\n",
    "        selected_sheet.set_dataframe(final_result, (x, y))\n",
    "    print(f\"uploaded * sheet name *{sheet_name}* in sheet_id *{sheet_id}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a66ff76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1359660/4200566825.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  final_result['updated_date'] = current_date_string\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/data-pipeline-s3-postgres/src/key1.json\n",
      "uploaded * sheet name *kt_pt_ct_report* in sheet_id *1VHNcEG4EExsROyhb4eTU0yl4JYktNOrBW9VeUCU25cw\n"
     ]
    }
   ],
   "source": [
    "df['keyword']=df['keyword'].str.replace(\"+\",\"*\")\n",
    "df['suggested_bid']=df['suggested_bid'].round(1)\n",
    "suggested_bid=df['suggested_bid']\n",
    "df=df.drop(\"suggested_bid\",axis=1)\n",
    "df.insert(14,\"suggested_bid\",suggested_bid)\n",
    "df['Acos_ratio']=df['Acos_ratio'].round(1)\n",
    "Acos_ratio=df['Acos_ratio']\n",
    "df=df.drop(\"Acos_ratio\",axis=1)\n",
    "df.insert(15,\"Acos_ratio\",Acos_ratio)\n",
    "df=df.sort_values(by='ad_spend_last_14_days',ascending=False)\n",
    "print_sheet(1,df, 'kt_pt_ct_report', '1VHNcEG4EExsROyhb4eTU0yl4JYktNOrBW9VeUCU25cw', x=1, y=1, mode=1,make_upper_case=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cc817a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
